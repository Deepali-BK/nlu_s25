{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "898baed3",
      "metadata": {
        "id": "898baed3"
      },
      "source": [
        "# HW 2: Efficient Fine-Tuning with BitFit?\n",
        "**Due: March 13, 11:30 AM**\n",
        "\n",
        "In this homework assignment, you will replicate [the BitFit experiments (Zaken et al., 2020)](https://aclanthology.org/2022.acl-short.1/). You will first use the [ü§ó Transformers framework](https://huggingface.co/docs/transformers/index) to fine-tune a [BERT$_\\text{tiny}$ model](https://huggingface.co/prajjwal1/bert-tiny) ([Turc et al., 2019](https://arxiv.org/abs/1908.08962); [Bhargava et al., 2021](https://aclanthology.org/2021.insights-1.18/)) on the IMDb dataset. You will then fine-tune the same model, but with all parameters frozen other than the bias terms. You will compare the two models on the following metrics: (1) their accuracy on the IMDb test set and (2) the number of parameters trained during fine-tuning.\n",
        "\n",
        "## Important: Read Before Starting\n",
        "\n",
        "In the following exercises, you will need to implement functions defined in the `train_model.py` and `test_model.py` scripts. **Please write all your code in those files.** You should not submit this notebook with your solutions, and we will not grade it if you do. Please be aware that code written in a Jupyter notebook may run differently when copied into Python modules.\n",
        "\n",
        "The outputs shown in this notebook are the outputs that you should get **when all problems have been completed correctly**. You may obtain different results if you attempt to run the code cells before you have completed the problem set, or if you have completed one or more problems incorrectly.\n",
        "\n",
        "For part of this assignment, you will be asked to fine-tune a BERT$_\\text{tiny}$ model on the IMDb dataset with hyperparameter tuning. **This will take several hours to run on a laptop with a CPU.** You may want to instead run your code on [Google Colaboratory](https://colab.research.google.com/) using a free GPU.\n",
        "\n",
        "To begin, please run the following `import` statements."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets evaluate optuna --quiet # install datasets if it is not included in your environment"
      ],
      "metadata": {
        "id": "-lVa6aKcgLyK"
      },
      "id": "-lVa6aKcgLyK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR27N8bsP0Rv",
        "outputId": "55f839b1-d146-412b-ea14-cdde4273ee17"
      },
      "id": "CR27N8bsP0Rv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOv3rEDKQH-9",
        "outputId": "8601cf59-0eee-498b-de9e-0b1d1d6ca12c"
      },
      "id": "GOv3rEDKQH-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Code for Problem 1 of HW 2.\n",
        "\"\"\"\n",
        "import pickle\n",
        "from typing import Any, Dict\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import optuna\n",
        "from datasets import Dataset, load_dataset\n",
        "#from transformers import BertTokenizerFast, BertForSequenceClassification, \\\n",
        "#    Trainer, TrainingArguments, EvalPrediction\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, \\\n",
        "    Trainer, TrainingArguments, EvalPrediction, EarlyStoppingCallback\n",
        "\n",
        "\n",
        "def preprocess_dataset(dataset: Dataset, tokenizer: BertTokenizerFast) \\\n",
        "        -> Dataset:\n",
        "    \"\"\"\n",
        "    Problem 1d: Implement this function.\n",
        "\n",
        "    Preprocesses a dataset using a Hugging Face Tokenizer and prepares\n",
        "    it for use in a Hugging Face Trainer.\n",
        "\n",
        "    :param dataset: A dataset\n",
        "    :param tokenizer: A tokenizer\n",
        "    :return: The dataset, prepreprocessed using the tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "    #return dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "    return dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    #raise NotImplementedError(\"Problem 1d has not been completed yet!\")\n",
        "\n",
        "\n",
        "def init_model(trial: Any, model_name: str, use_bitfit: bool = False) -> \\\n",
        "        BertForSequenceClassification:\n",
        "    \"\"\"\n",
        "    Problem 2a: Implement this function.\n",
        "\n",
        "    This function should be passed to your Trainer's model_init keyword\n",
        "    argument. It will be used by the Trainer to initialize a new model\n",
        "    for each hyperparameter tuning trial. Your implementation of this\n",
        "    function should support training with BitFit by freezing all non-\n",
        "    bias parameters of the initialized model.\n",
        "\n",
        "    :param trial: This parameter is required by the Trainer, but it will\n",
        "        not be used for this problem. Please ignore it\n",
        "    :param model_name: The identifier listed in the Hugging Face Model\n",
        "        Hub for the pre-trained model that will be loaded\n",
        "    :param use_bitfit: If True, then all parameters will be frozen other\n",
        "        than bias terms\n",
        "    :return: A newly initialized pre-trained Transformer classifier\n",
        "    \"\"\"\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    if use_bitfit:\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"bias\" not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    return model\n",
        "    #raise NotImplementedError(\"Problem 2a has not been completed yet!\")\n",
        "\n",
        "#new\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    \"\"\"Computes accuracy for evaluation.\"\"\"\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "#new\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def init_trainer(model_name: str, train_data: Dataset, val_data: Dataset,\n",
        "                 use_bitfit: bool = False) -> Trainer:\n",
        "    \"\"\"\n",
        "    Prolem 2b: Implement this function.\n",
        "\n",
        "    Creates a Trainer object that will be used to fine-tune a BERT-tiny\n",
        "    model on the IMDb dataset. The Trainer should fulfill the criteria\n",
        "    listed in the problem set.\n",
        "\n",
        "    :param model_name: The identifier listed in the Hugging Face Model\n",
        "        Hub for the pre-trained model that will be fine-tuned\n",
        "    :param train_data: The training data used to fine-tune the model\n",
        "    :param val_data: The validation data used for hyperparameter tuning\n",
        "    :param use_bitfit: If True, then all parameters will be frozen other\n",
        "        than bias terms\n",
        "    :return: A Trainer used for training\n",
        "    \"\"\"\n",
        "    #output_dir = get_new_run_directory() #new\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./checkpoints\",\n",
        "        #output_dir = output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=3,\n",
        "        metric_for_best_model=\"accuracy\", greater_is_better=True,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=lambda trial: init_model(trial, model_name, use_bitfit),\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=val_data,\n",
        "        compute_metrics=compute_metrics,  # Added metric computation\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "    )\n",
        "\n",
        "    return trainer\n",
        "\n",
        "    #raise NotImplementedError(\"Problem 2b has not been completed yet!\")\n",
        "\n",
        "\n",
        "def hyperparameter_search_settings() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Problem 2c: Implement this function.\n",
        "\n",
        "    Returns keyword arguments passed to Trainer.hyperparameter_search.\n",
        "    Your hyperparameter search must satisfy the criteria listed in the\n",
        "    problem set.\n",
        "\n",
        "    :return: Keyword arguments for Trainer.hyperparameter_search\n",
        "    \"\"\"\n",
        "\n",
        "    return {\n",
        "        \"direction\": \"maximize\",\n",
        "        \"n_trials\": 20,  # Increased trials for better tuning\n",
        "        \"hp_space\": lambda trial: {\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-5, log=True),\n",
        "            \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
        "            \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 6),\n",
        "            \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    #raise NotImplementedError(\"Problem 2c has not been completed yet!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":  # Use this script to train your model\n",
        "    model_name = \"prajjwal1/bert-tiny\"\n",
        "\n",
        "    # Load IMDb dataset and create validation split\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    split = imdb[\"train\"].train_test_split(.2, seed=3463)\n",
        "    imdb[\"train\"] = split[\"train\"]\n",
        "    imdb[\"val\"] = split[\"test\"]\n",
        "    del imdb[\"unsupervised\"]\n",
        "    del imdb[\"test\"]\n",
        "\n",
        "    # Preprocess the dataset for the trainer\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "    imdb[\"train\"] = preprocess_dataset(imdb[\"train\"], tokenizer)\n",
        "    imdb[\"val\"] = preprocess_dataset(imdb[\"val\"], tokenizer)\n",
        "\n",
        "    # Set up trainer\n",
        "    trainer = init_trainer(model_name, imdb[\"train\"], imdb[\"val\"],\n",
        "                           use_bitfit=True)\n",
        "\n",
        "    # Train and save the best hyperparameters\n",
        "\n",
        "    best = trainer.hyperparameter_search(**hyperparameter_search_settings())\n",
        "    \"\"\"\n",
        "    with open(\"train_results.p\", \"wb\") as f:\n",
        "        pickle.dump(best, f)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(\"train_results.p\", \"rb\") as f:\n",
        "            best = pickle.load(f)  # ‚úÖ This ensures `f` is properly defined\n",
        "            print(\"Loaded best hyperparameters:\", best)\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è train_results.p not found! Make sure you have run training first.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è An error occurred while loading train_results.p: {e}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3536c7e240f7427d938900f54b1148df",
            "f346cda3495c4e46a1f2552b1ccb273d",
            "22ebd04f94714762bf99259940a90182",
            "cc6a73cf3b204c0aa43d8b620a79dfa8",
            "e225bb9f37014b968e53d90aa3c0523e",
            "b3ac1b8d6eb74354a4eb481d2b832a89",
            "ef199a27322647af9531d6be0a16dbf1",
            "e87d04a7040e4b808bbf02914cd69838",
            "5052adb85ce0412ba7af7becb60eafef",
            "6e74a9cce5f4424b9ab9133edb43c241",
            "be39d4cedfa4464a8483865cb90cdf24",
            "79998ff89c65408f81fcc1d524991e98",
            "c3b79b81060b444a887ab72c96f5e0f5",
            "dd36b9658601403b8f91fcb2f2209da7",
            "bccc145a51494b8aa402b4d1762b1c22",
            "d2dee6f8a19742ee8e51e0a0e5430d11",
            "366a1084b9be425aa793cacbcd68d727",
            "d8409117e70e4b51baa7359c77a453fd",
            "0bc0301dcfb64a859ab696f2a2650f44",
            "7616d6b82955427794261b488f4d3de0",
            "9c575db410284dc5867e4a45bde8116e",
            "81305b90178c475e8c544af00e794220"
          ]
        },
        "id": "-FM_C3WsCex4",
        "outputId": "22a20e86-af6c-4aaf-bfba-1fbdfa2f3351"
      },
      "id": "-FM_C3WsCex4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3536c7e240f7427d938900f54b1148df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79998ff89c65408f81fcc1d524991e98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-03-15 19:10:17,430] A new study created in memory with name: no-name-f087f4be-11cc-45c4-8136-558af571f48d\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sleek-galaxy-26</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/prpzyxsp' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/prpzyxsp</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_190741-prpzyxsp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_191018-gp5yu04j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/gp5yu04j' target=\"_blank\">cool-snowflake-27</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/gp5yu04j' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/gp5yu04j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 03:40, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.709800</td>\n",
              "      <td>0.712794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 19:14:01,324] Trial 0 finished with value: 0.71279376745224 and parameters: {'learning_rate': 2.239985538470037e-06, 'num_train_epochs': 1, 'seed': 26, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.71279376745224.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.71279</td></tr><tr><td>eval/runtime</td><td>20.3165</td></tr><tr><td>eval/samples_per_second</td><td>246.105</td></tr><tr><td>eval/steps_per_second</td><td>30.763</td></tr><tr><td>total_flos</td><td>6352435200000.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>625</td></tr><tr><td>train/grad_norm</td><td>0.10848</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7098</td></tr><tr><td>train_loss</td><td>0.70893</td></tr><tr><td>train_runtime</td><td>222.2936</td></tr><tr><td>train_samples_per_second</td><td>89.971</td></tr><tr><td>train_steps_per_second</td><td>2.812</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cool-snowflake-27</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/gp5yu04j' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/gp5yu04j</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_191018-gp5yu04j/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_191402-ymsabik5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/ymsabik5' target=\"_blank\">woven-meadow-28</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/ymsabik5' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/ymsabik5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 03:49, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.700600</td>\n",
              "      <td>0.699346</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 19:17:54,400] Trial 1 finished with value: 0.6993458271026611 and parameters: {'learning_rate': 1.8654145377087265e-06, 'num_train_epochs': 1, 'seed': 1, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 0.71279376745224.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÉ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÑ‚ñÖ‚ñà‚ñÅ‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.69935</td></tr><tr><td>eval/runtime</td><td>20.279</td></tr><tr><td>eval/samples_per_second</td><td>246.561</td></tr><tr><td>eval/steps_per_second</td><td>30.82</td></tr><tr><td>total_flos</td><td>6352435200000.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>2500</td></tr><tr><td>train/grad_norm</td><td>0.33003</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.7006</td></tr><tr><td>train_loss</td><td>0.70033</td></tr><tr><td>train_runtime</td><td>231.4217</td></tr><tr><td>train_samples_per_second</td><td>86.422</td></tr><tr><td>train_steps_per_second</td><td>10.803</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">woven-meadow-28</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/ymsabik5' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/ymsabik5</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_191402-ymsabik5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_191755-4jqnrxrg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/4jqnrxrg' target=\"_blank\">bumbling-resonance-29</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/4jqnrxrg' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/4jqnrxrg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 04:21, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.697700</td>\n",
              "      <td>0.698429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 19:22:19,541] Trial 2 finished with value: 0.6984289884567261 and parameters: {'learning_rate': 2.2667536963828638e-05, 'num_train_epochs': 1, 'seed': 3, 'per_device_train_batch_size': 4}. Best is trial 0 with value: 0.71279376745224.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÜ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñà‚ñÑ‚ñÅ‚ñá‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.69843</td></tr><tr><td>eval/runtime</td><td>21.229</td></tr><tr><td>eval/samples_per_second</td><td>235.527</td></tr><tr><td>eval/steps_per_second</td><td>29.441</td></tr><tr><td>total_flos</td><td>6352435200000.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>5000</td></tr><tr><td>train/grad_norm</td><td>0.16421</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.6977</td></tr><tr><td>train_loss</td><td>0.69877</td></tr><tr><td>train_runtime</td><td>263.3432</td></tr><tr><td>train_samples_per_second</td><td>75.947</td></tr><tr><td>train_steps_per_second</td><td>18.987</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bumbling-resonance-29</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/4jqnrxrg' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/4jqnrxrg</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_191755-4jqnrxrg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_192220-2qhjdg9l</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/2qhjdg9l' target=\"_blank\">good-spaceship-30</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/2qhjdg9l' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/2qhjdg9l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [626/626 06:52, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.695800</td>\n",
              "      <td>0.694546</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 19:29:15,951] Trial 3 finished with value: 0.694546103477478 and parameters: {'learning_rate': 8.577376442502214e-05, 'num_train_epochs': 2, 'seed': 27, 'per_device_train_batch_size': 64}. Best is trial 0 with value: 0.71279376745224.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÖ‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÖ‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.69455</td></tr><tr><td>eval/runtime</td><td>21.7832</td></tr><tr><td>eval/samples_per_second</td><td>229.535</td></tr><tr><td>eval/steps_per_second</td><td>28.692</td></tr><tr><td>total_flos</td><td>12704870400000.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>626</td></tr><tr><td>train/grad_norm</td><td>0.08006</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.6958</td></tr><tr><td>train_loss</td><td>0.6955</td></tr><tr><td>train_runtime</td><td>414.7837</td></tr><tr><td>train_samples_per_second</td><td>96.436</td></tr><tr><td>train_steps_per_second</td><td>1.509</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">good-spaceship-30</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/2qhjdg9l' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/2qhjdg9l</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_192220-2qhjdg9l/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_192916-6eamy4jd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/6eamy4jd' target=\"_blank\">stilted-wood-31</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/6eamy4jd' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/6eamy4jd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 13:42, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.701600</td>\n",
              "      <td>0.702047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.698600</td>\n",
              "      <td>0.698858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.695500</td>\n",
              "      <td>0.697093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.694000</td>\n",
              "      <td>0.696521</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 19:43:01,305] Trial 4 finished with value: 0.6965214014053345 and parameters: {'learning_rate': 1.346226900769052e-05, 'num_train_epochs': 4, 'seed': 11, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.71279376745224.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÑ‚ñÑ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.69652</td></tr><tr><td>eval/runtime</td><td>21.7063</td></tr><tr><td>eval/samples_per_second</td><td>230.348</td></tr><tr><td>eval/steps_per_second</td><td>28.793</td></tr><tr><td>total_flos</td><td>25409740800000.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>2500</td></tr><tr><td>train/grad_norm</td><td>0.11075</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.694</td></tr><tr><td>train_loss</td><td>0.69681</td></tr><tr><td>train_runtime</td><td>823.792</td></tr><tr><td>train_samples_per_second</td><td>97.112</td></tr><tr><td>train_steps_per_second</td><td>3.035</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stilted-wood-31</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/6eamy4jd' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/6eamy4jd</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_192916-6eamy4jd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_194302-52sr2oeu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/52sr2oeu' target=\"_blank\">worthy-bird-32</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/52sr2oeu' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/52sr2oeu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2500/10000 03:50 < 11:33, 10.82 it/s, Epoch 1/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>0.695462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 19:46:54,331] Trial 5 pruned. \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñÑ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.69546</td></tr><tr><td>eval/runtime</td><td>21.8346</td></tr><tr><td>eval/samples_per_second</td><td>228.995</td></tr><tr><td>eval/steps_per_second</td><td>28.624</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>2500</td></tr><tr><td>train/grad_norm</td><td>0.27045</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6941</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">worthy-bird-32</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/52sr2oeu' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/52sr2oeu</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_194302-52sr2oeu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_194655-9v4sq2q2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/9v4sq2q2' target=\"_blank\">northern-eon-33</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/9v4sq2q2' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/9v4sq2q2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7500/7500 11:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.714400</td>\n",
              "      <td>0.710316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.715900</td>\n",
              "      <td>0.707670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.715200</td>\n",
              "      <td>0.706895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 19:58:30,208] Trial 6 finished with value: 0.7068954110145569 and parameters: {'learning_rate': 1.982229058393526e-06, 'num_train_epochs': 3, 'seed': 24, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 0.71279376745224.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñá‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñá‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÑ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.7069</td></tr><tr><td>eval/runtime</td><td>20.5075</td></tr><tr><td>eval/samples_per_second</td><td>243.813</td></tr><tr><td>eval/steps_per_second</td><td>30.477</td></tr><tr><td>total_flos</td><td>19057305600000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>7500</td></tr><tr><td>train/grad_norm</td><td>0.3868</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.7152</td></tr><tr><td>train_loss</td><td>0.71712</td></tr><tr><td>train_runtime</td><td>694.2139</td></tr><tr><td>train_samples_per_second</td><td>86.429</td></tr><tr><td>train_steps_per_second</td><td>10.804</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">northern-eon-33</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/9v4sq2q2' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/9v4sq2q2</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_194655-9v4sq2q2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_195831-8vichwwc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/8vichwwc' target=\"_blank\">fiery-donkey-34</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/8vichwwc' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/8vichwwc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/3750 03:37 < 07:15, 5.75 it/s, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.699000</td>\n",
              "      <td>0.696094</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 20:02:09,924] Trial 7 pruned. \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.69609</td></tr><tr><td>eval/runtime</td><td>20.5314</td></tr><tr><td>eval/samples_per_second</td><td>243.529</td></tr><tr><td>eval/steps_per_second</td><td>30.441</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>1250</td></tr><tr><td>train/grad_norm</td><td>0.21838</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.699</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fiery-donkey-34</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/8vichwwc' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/8vichwwc</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_195831-8vichwwc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_200211-de35j247</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/de35j247' target=\"_blank\">brisk-moon-35</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/de35j247' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/de35j247</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1252' max='1252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1252/1252 13:45, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.700698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.702300</td>\n",
              "      <td>0.700479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.702300</td>\n",
              "      <td>0.700338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.701400</td>\n",
              "      <td>0.700295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 20:15:59,056] Trial 8 finished with value: 0.700294554233551 and parameters: {'learning_rate': 3.231119950187164e-06, 'num_train_epochs': 4, 'seed': 8, 'per_device_train_batch_size': 64}. Best is trial 0 with value: 0.71279376745224.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÜ‚ñá‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÜ‚ñá‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.70029</td></tr><tr><td>eval/runtime</td><td>20.8462</td></tr><tr><td>eval/samples_per_second</td><td>239.852</td></tr><tr><td>eval/steps_per_second</td><td>29.982</td></tr><tr><td>total_flos</td><td>26680227840000.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>1252</td></tr><tr><td>train/grad_norm</td><td>0.0631</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7014</td></tr><tr><td>train_loss</td><td>0.70163</td></tr><tr><td>train_runtime</td><td>827.2794</td></tr><tr><td>train_samples_per_second</td><td>96.703</td></tr><tr><td>train_steps_per_second</td><td>1.513</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">brisk-moon-35</strong> at: <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/de35j247' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/de35j247</a><br> View project at: <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_200211-de35j247/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_201600-0yy8p43p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/0yy8p43p' target=\"_blank\">twilight-glade-36</a></strong> to <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/db5144-new-york-university/huggingface' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/db5144-new-york-university/huggingface/runs/0yy8p43p' target=\"_blank\">https://wandb.ai/db5144-new-york-university/huggingface/runs/0yy8p43p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/626 03:27 < 03:29, 1.50 it/s, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.692833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-15 20:19:29,827] Trial 9 pruned. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Code for Problem 1 of HW 2.\n",
        "\"\"\"\n",
        "import pickle\n",
        "\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, \\\n",
        "    Trainer, TrainingArguments\n",
        "\n",
        "from train_model import preprocess_dataset\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy for evaluation.\"\"\"\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "def init_tester(directory: str) -> Trainer:\n",
        "    \"\"\"\n",
        "    Prolem 2b: Implement this function.\n",
        "\n",
        "    Creates a Trainer object that will be used to test a fine-tuned\n",
        "    model on the IMDb test set. The Trainer should fulfill the criteria\n",
        "    listed in the problem set.\n",
        "\n",
        "    :param directory: The directory where the model being tested is\n",
        "        saved\n",
        "    :return: A Trainer used for testing\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=directory,\n",
        "        per_device_eval_batch_size=8,\n",
        "        do_eval=True,\n",
        "    )\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(directory)\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(directory)\n",
        "\n",
        "    def compute_metrics(eval_pred: EvalPrediction):\n",
        "        metric = evaluate.load(\"accuracy\")\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    return trainer\n",
        "    \"\"\"\n",
        "    model = BertForSequenceClassification.from_pretrained(directory, num_labels=2)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./test_results\",\n",
        "        per_device_eval_batch_size=8,\n",
        "        do_train=False,  # Disable training\n",
        "        do_eval=True,    # Enable evaluation\n",
        "        evaluation_strategy=\"no\", #new\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        #eval_dataset=test_data,\n",
        "        #eval_dataset=imdb, #new\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "    #raise NotImplementedError(\"Problem 2b has not been completed yet!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":  # Use this script to test your model\n",
        "    model_name = \"prajjwal1/bert-tiny\"\n",
        "\n",
        "    # Load IMDb dataset\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    del imdb[\"train\"]\n",
        "    del imdb[\"unsupervised\"]\n",
        "\n",
        "    # Preprocess the dataset for the tester\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "    imdb[\"test\"] = preprocess_dataset(imdb[\"test\"], tokenizer)\n",
        "\n",
        "    # Set up tester\n",
        "    tester = init_tester(\"path_to_your_best_model\")\n",
        "\n",
        "    # Test\n",
        "    results = tester.predict(imdb[\"test\"])\n",
        "    with open(\"test_results.p\", \"wb\") as f:\n",
        "        pickle.dump(results, f)\n"
      ],
      "metadata": {
        "id": "C4NGa15-Cg0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "9a9f08a6-22ad-4bd1-970c-a716261be73e"
      },
      "id": "C4NGa15-Cg0b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'train_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2246db9dc74a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'train_model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2593b3aa",
      "metadata": {
        "id": "2593b3aa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections.abc import Iterable\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Model and tokenizer from ü§ó Transformers\n",
        "from transformers import AutoModelForSequenceClassification, \\\n",
        "    BertForSequenceClassification, BertTokenizerFast\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code you will write for this assignment\n",
        "from train_model import init_model, preprocess_dataset, init_trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "feBcOSfnTSKe",
        "outputId": "9e392263-13a1-4b58-c64c-e735801e9c5c"
      },
      "id": "feBcOSfnTSKe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'train_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-41ec15d560ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Code you will write for this assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'train_model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from test_model import init_tester"
      ],
      "metadata": {
        "id": "IZgTSz0Vd3DC"
      },
      "id": "IZgTSz0Vd3DC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5110c1a8",
      "metadata": {
        "id": "5110c1a8"
      },
      "source": [
        "## Problem 1: Setup (30 Points in Total)\n",
        "\n",
        "In this assignment, you will fine-tune a pre-trained Transformer model using libraries provided by [Hugging Face](https://huggingface.co/) (whose name is usually styled using the emoji ü§ó). You have already been exposed to Hugging Face in lab, where you used the [ü§ó Datasets](https://huggingface.co/docs/datasets/index) library to load the IMDb dataset and the [ü§ó Transformers](https://huggingface.co/docs/transformers/index) library to load a pre-trained BERT$_\\text{tiny}$ model. In the following problems, additionally use the [ü§ó Evaluate](https://huggingface.co/docs/evaluate/index) library, which provides evaluation metrics such as accuracy and F1.\n",
        "\n",
        "For several parts of this problem, you will need to refer to the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training) for guidance.\n",
        "\n",
        "### Problem 1a: Understand the ü§ó Transformers Library (No Submission, 0 Points)\n",
        "\n",
        "ü§ó Transformers is imported into Python via the name `transformers`. Please find the import statements from ü§ó Transformers in the code cell above.\n",
        "\n",
        "ü§ó Transformers comes with a number of different Transformer architectures, as well as [the Model Hub, a repository of pre-trained model parameters](https://huggingface.co/models). A pre-trained model is loaded by calling the model architecture's `.from_pretrained` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14fd3b9",
      "metadata": {
        "id": "e14fd3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "3d49d773b0264506a7b9de01cea41515",
            "84d34c1b84ca401cbdecaae0e10c03e7",
            "b0b55fb901dc417bb3d82a8f04117542",
            "f00ebdf7935f47a5b52d822baeba457a",
            "e967a3f3887c436a8c9e6d9c29118c55",
            "93b184ea6e1742418eb753c36c3846c0",
            "34c7498f18c64daca9b737675ef18b93",
            "33ea8d146f34405499898213d038b954",
            "e82a42390f8e4f04bf4b9858f182884f",
            "e07049126ad44ed59bf02bc92bd8a07e",
            "e41277945b2a40e7a474b91f23a62794",
            "c7186a97aa114e4992b3a663261cf0b3",
            "b7caff86feca46b9baa7260d8bfc262e",
            "ae79f4eebc504268bc4279feb74a3b9a",
            "24258277f98b4bc08dfe15a2faf168b3",
            "6811fa68fbd74e4daf5965429134a51b",
            "646df8f2ab0843bfb8d6d240de9dc173",
            "3986b80c712949809829a70a20f20dc1",
            "0b636f8e343b46d0a309658991cd7c36",
            "7c264b7eed164e67bb55a6da4a59f6b5",
            "39e50207c331437cbb93fd30d07d45bc",
            "6a6229d5572e4551b87824bb7ddba3d1"
          ]
        },
        "outputId": "49169ac1-e87c-4853-deda-2fbe368840c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d49d773b0264506a7b9de01cea41515"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7186a97aa114e4992b3a663261cf0b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f653226",
      "metadata": {
        "id": "7f653226"
      },
      "source": [
        "The code above loads a Transformer classifier consisting of a pre-trained BERT$_\\text{base}$ encoder with case-insensitive vocabulary and a randomly initialized 2-layer MLP decoder with tanh activation. The choice of this particular set of pre-trained parameters is specified by the identifier `'bert-base-uncased'`, which is passed to the first parameter of `.from_pretrained`. Different pre-trained weights can be loaded by passing a different identifier to `.from_pretrained`. The following code loads the BERT$_\\text{tiny}$ model from [Turc et al. (2019)](https://arxiv.org/abs/1908.08962) and [Bhargava et al. (2021)](https://aclanthology.org/2021.insights-1.18/), which you will be fine-tuning in this assignment. (The `/` indicates that this is a user-submitted model, uploaded by the user [`prajjwal1`](https://huggingface.co/prajjwal1).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f491da1d",
      "metadata": {
        "id": "f491da1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac15bb2c-4457-4c3e-a6a8-45299a3c7c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"prajjwal1/bert-tiny\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cbed54d",
      "metadata": {
        "id": "2cbed54d"
      },
      "source": [
        "In order to load a model using the code above, you would have to know that BERT$_{\\text{tiny}}$'s architecture is implemented using the same class as BERT$_{\\text{base}}$. This is not true in general, however. For instance, if you wanted to initialize a RoBERTa classifier instead of a BERT classifier, you would need to call `RobertaForSequenceClassification.from_pretrained` instead of `BertForSequenceClassification.from_pretrained`. When you don't know which class implements the architecture of pre-trained model you want to load, you can use the `AutoModelForSequenceClassification` class ([and equivalent classes for other tasks](https://huggingface.co/docs/transformers/model_doc/auto)), which will figure out which class to instantiate based on the pre-trained weights you would like to load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cf7838",
      "metadata": {
        "id": "33cf7838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8beec57a-d195-4342-a53d-7d8d0f97b255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# This code does exactly the same thing as the previous code cell\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"prajjwal1/bert-tiny\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9b9706",
      "metadata": {
        "id": "2d9b9706"
      },
      "source": [
        "In addition to models, ü§ó Transformers also provides tokenizers that implement a full processing pipeline similar to what you implemented in HW 2. You can load the appropriate tokenizer for your model using a `.from_pretrained` method, just as you did with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c374e9",
      "metadata": {
        "id": "a5c374e9"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"prajjwal1/bert-tiny\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf262f25",
      "metadata": {
        "id": "cf262f25"
      },
      "source": [
        "As we saw in lab, the tokenizer object can be called as a function. Doing so will return a fully processed input, ready to be passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2388ba95",
      "metadata": {
        "id": "2388ba95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0cd31e-935a-428e-dbf4-50ab3e2afae8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 7592, 2088,  999,  102,    0],\n",
              "        [ 101, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
              "        [1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Because ü§ó Transformers supports multiple deep learning libraries, you will\n",
        "# need to use the keyword parameter return_tensors in order to indicate that\n",
        "# you want your inputs to be returned in PyTorch format.\n",
        "inputs = tokenizer([\"Hello world!\", \"How are you?\"], padding=True,\n",
        "                   return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f381cc9",
      "metadata": {
        "id": "1f381cc9"
      },
      "source": [
        "The inputs returned by the tokenizer are passed to the model via [dictionary unpacking](https://realpython.com/python-kwargs-and-args/). The output of the model is structured, with various kinds of information provided depending on keyword arguments passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f1cba4",
      "metadata": {
        "id": "63f1cba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1608c616-7b9a-4e0e-b7cc-fd1f0533d792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0604,  0.3207],\n",
            "        [-0.1486,  0.2225]]), hidden_states=None, attentions=None)\n",
            "\n",
            "tensor([[-0.0604,  0.3207],\n",
            "        [-0.1486,  0.2225]])\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(outputs, end=\"\\n\\n\")\n",
        "\n",
        "# Use the dot operator to access parts of the output\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de67f0e9",
      "metadata": {
        "id": "de67f0e9"
      },
      "source": [
        "### Problem 1b: Understand BERT Inputs (Written, 10 Points)\n",
        "\n",
        "Look at the tokenized inputs from two code cells above. The inputs are represented as a dict with three keys: `'input_ids'`, `'token_type_ids'`, and `'attention_mask'`. What do each of those three inputs represent? Please consult the [original BERT paper (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805) for guidance.\n",
        "\n",
        "### Problem 1c: Understand BERT Hyperparameters (Written, 10 Points)\n",
        "\n",
        "For this assignment, you will perform hyperparameter tuning for the BERT$_\\text{tiny}$ model using the same procedure as in the [original paper](https://arxiv.org/abs/1908.08962). Their hyperparameter tuning procedure is documented in the [official BERT GitHub repository](https://github.com/google-research/bert) under the heading \"**\\*\\*\\*\\*\\*New March 11th, 2020: Smaller BERT Models\\*\\*\\*\\*\\***.\" Please read this documentation and describe how hyperparameter tuning was performed for the GLUE benchmark.\n",
        "\n",
        "### Problem 1d: Prepare Dataset (Code, 10 Points)\n",
        "\n",
        "As in lab, we will be using the IMDb dataset provided by ü§ó Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406d2800",
      "metadata": {
        "id": "406d2800"
      },
      "outputs": [],
      "source": [
        "# Load IMDb dataset and create validation split\n",
        "imdb = load_dataset(\"imdb\")\n",
        "split = imdb[\"train\"].train_test_split(.2, seed=3463)\n",
        "imdb[\"train\"] = split[\"train\"]\n",
        "imdb[\"val\"] = split[\"test\"]\n",
        "del imdb[\"unsupervised\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a8cb0e7",
      "metadata": {
        "id": "3a8cb0e7"
      },
      "source": [
        "The ü§ó Transformers fine-tuning API expects datasets to be pre-processed through the following steps.\n",
        "- All input texts should be tokenized.\n",
        "- BERT models have a maximum input length, and all inputs need to be truncated to this length.\n",
        "- Inputs shorter than the maximum input length should be padded to this length.\n",
        "- The pre-processed inputs do not need to be in the form of PyTorch tensors.\n",
        "\n",
        "These steps are performed by the `preprocess_dataset` function in `run_experiment.py`, which you will implement for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efacd457",
      "metadata": {
        "id": "efacd457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "ac654993-1f03-4c94-d484-3aa5990ad771"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3491c4718f3d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Visualize the preprocessed dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "imdb[\"train\"] = preprocess_dataset(imdb[\"train\"], tokenizer)\n",
        "imdb[\"val\"] = preprocess_dataset(imdb[\"val\"], tokenizer)\n",
        "imdb[\"test\"] = preprocess_dataset(imdb[\"test\"], tokenizer)\n",
        "\n",
        "# Visualize the preprocessed dataset\n",
        "for k, v in imdb[\"val\"][:2].items():\n",
        "    print(\"{}:\\n{}\\n{}\\n\".format(k, type(v),\n",
        "                                 [item[:20] if isinstance(item, Iterable) else\n",
        "                                 item for item in v[:5]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ac12ac",
      "metadata": {
        "id": "b8ac12ac"
      },
      "source": [
        "Please base your implementation on the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training), and please consult [Appendix A.2 of the BERT paper](https://arxiv.org/abs/1810.04805) to find out what the maximum input length should be.\n",
        "\n",
        "## Problem 2: Implement Experiment (50 Points in Total)\n",
        "### Problem 2a: Freeze Non-Bias Weights (Code, 10 Points)\n",
        "\n",
        "At the end of this assignment, you will be comparing a BERT$_{\\text{tiny}}$ model fine-tuned using BitFit to a BERT$_{\\text{tiny}}$ model fine-tuned _without_ BitFit. To run that experiment, you will need to support freezing all non-bias parameters of the model. To do this, please implement the `init_model` function, illustrated below. This function should load a pre-trained BERT classifier model from the Hugging Face Model Hub and optionally freeze non-bias parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8727100e",
      "metadata": {
        "id": "8727100e"
      },
      "outputs": [],
      "source": [
        "# The first parameter is unused; we just pass None to it\n",
        "model = init_model(None, \"prajjwal1/bert-tiny\", use_bitfit=True)\n",
        "\n",
        "# Check if weight matrix is frozen\n",
        "print(model.bert.encoder.layer[0].attention.self.query.weight.requires_grad)\n",
        "\n",
        "# Check if bias term is frozen\n",
        "print(model.bert.encoder.layer[0].attention.self.query.bias.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "173b6886",
      "metadata": {
        "id": "173b6886"
      },
      "source": [
        "**Hint:** Please consult the [documentation for the function `nn.Module.named_parameters`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters).\n",
        "\n",
        "### Problem 2b: Set Up Trainer and Tester (Code, 20 Points)\n",
        "\n",
        "ü§ó Transformers provides a [`Trainer` object](https://huggingface.co/docs/transformers/main_classes/trainer) that implements training and testing a neural network. For this problem, please implement the functions `init_trainer` in `train_model.py` and `init_tester` in `test_model.py`, which will set up the `Trainer`s used to train and test your model, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06758f89",
      "metadata": {
        "id": "06758f89"
      },
      "outputs": [],
      "source": [
        "# Creates a Trainer from a Hugging Face Model Hub identifier\n",
        "trainer = init_trainer(\"prajjwal1/bert-tiny\", imdb[\"train\"], imdb[\"val\"])\n",
        "\n",
        "# Train using the trainer\n",
        "trainer.train()\n",
        "\n",
        "# Change this to whichever checkpoint you want to evalaute\n",
        "eval_checkpoint_directory = \"checkpoints/run-13/checkpoint-1252\"\n",
        "\n",
        "# Creates a Trainer to test a Hugging Face saved model\n",
        "tester = init_tester(eval_checkpoint_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a67fcde8",
      "metadata": {
        "id": "a67fcde8"
      },
      "source": [
        "Your `init_trainer` function needs to support the following.\n",
        "- The training configuration (total number of epochs, early stopping criteria if any) must match your answer for Problem 1c.\n",
        "- Your `Trainer` needs to save the model obtained during each training run to a folder called `checkpoints`.\n",
        "- You should leave the `model` keyword parameter blank and instead pass an argument to the `model_init` keyword parameter.\n",
        "- It should evaluate models based on accuracy.\n",
        "\n",
        "Your `init_tester` function needs to support the following.\n",
        "- The `Trainer` should only support testing and not traiing.\n",
        "- It should evaluate models based on accuracy.\n",
        "\n",
        "\n",
        "Please use the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training) as well as [this forum post](https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378/3) for guidance. You may need to create new functions for this problem, and you may find it useful to learn about [lambda expressions](https://realpython.com/python-lambda/) if you don't know about them already.\n",
        "\n",
        "### Problem 2c: Set Up Hyperparameter Tuning (Code, 20 Points)\n",
        "\n",
        "Finally, to complete the experiment setup, you will implement hyperparameter tuning using the [Optuna](https://optuna.org/) framework. Optuna is integrated with ü§ó Transformers, and it can be invoked via the `Trainer.hyperparameter_search` method. Please implement the function `hyperparameter_search_settings` in `train_model.py` by returing the correct keyword arguments for `Trainer.hyperparameter_search`. (Observe that, at the end of `train_model.py`, these keyword arguments are passed to `Trainer.hyperparameter_search` via dictionary unpacking.)  \n",
        "\n",
        "Your code should support the following requirements.\n",
        "- Your hyperparameter tuning configuration must match your answer for Problem 1c.\n",
        "- You must use Optuna for hyperparameter tuning.\n",
        "- You must indicate to Optuna that the hyperparameter search should maximize accuracy.\n",
        "\n",
        "Please use the following resources for guidance.\n",
        "- [The Hugging Face tutorial on hyperparameter tuning](https://huggingface.co/docs/transformers/hpo_train)\n",
        "- [The documentation for `Trainer.hyperparameter_search`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.Trainer.hyperparameter_search)\n",
        "- [The documentation for Optuna's `GridSampler`](https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.samplers.GridSampler.html)\n",
        "\n",
        "## Problem 3: Run Experiment (20 Points in Total)\n",
        "\n",
        "To complete the assignment, you will now run your code and report on the results. It is recommended that you run your code on [Google Colaboratory](https://colab.research.google.com/) using a free GPU.\n",
        "\n",
        "### Problem 3a: Train Models (Code and Written, 10 Points)\n",
        "\n",
        "Please now run the following experimental procedure by running `train_model.py` as a Python script:\n",
        "- first, fine-tune a BERT$_{\\text{tiny}}$ model on the IMDb dataset _with_ BitFit;\n",
        "- then, fine-tune a BERT$_{\\text{tiny}}$ model on the IMDb dataset _without_ BitFit.\n",
        "\n",
        "The `train_model.py` script should create a Pickle object containing information about the best hyperparameters found during hyperparameter tuning. Please submit this object, using the filenames `train_results_with_bitfit.p` and `train_results_without_bitfit.p` for your two training runs, respectively. Please also report the highest validation accuracy attained in each of your two training runs, as well as the hyperparameters used in those trials. Please format these results as a table such as the following.\n",
        "\n",
        "| | Validation Accuracy | Learning Rate | Batch Size |\n",
        "|---|---|---|---|\n",
        "| Without BitFit | | | |\n",
        "| With BitFit | | | |\n",
        "\n",
        "### Problem 3b: Test Models and Report Results (Code and Written, 10 Points)\n",
        "\n",
        "For each of your two training runs, please test the model that attained the best validation accuracy across all hyperparameter tuning trials. You may do so by running the `test_model.py` script. Once testing is complete, please report your results in the form of a table such as the following.\n",
        "\n",
        "| | # Trainable Parameters | Test Accuracy |\n",
        "|---|---|---|\n",
        "| Without BitFit | | |\n",
        "| With BitFit | | |\n",
        "\n",
        "The `test_model.py` script should create a Pickle object containing information about test results. Please submit this object, using the filenames `test_results_with_bitfit.p` and `test_results_without_bitfit.p` for your two tests.\n",
        "\n",
        "Finally, please comment on your results. How do they compare to the results reported by Zaken et al. (2020)? What does this say about BitFit and its applicability to other pre-trained Transformers?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3536c7e240f7427d938900f54b1148df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f346cda3495c4e46a1f2552b1ccb273d",
              "IPY_MODEL_22ebd04f94714762bf99259940a90182",
              "IPY_MODEL_cc6a73cf3b204c0aa43d8b620a79dfa8"
            ],
            "layout": "IPY_MODEL_e225bb9f37014b968e53d90aa3c0523e"
          }
        },
        "f346cda3495c4e46a1f2552b1ccb273d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3ac1b8d6eb74354a4eb481d2b832a89",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ef199a27322647af9531d6be0a16dbf1",
            "value": "Map:‚Äá100%"
          }
        },
        "22ebd04f94714762bf99259940a90182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e87d04a7040e4b808bbf02914cd69838",
            "max": 20000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5052adb85ce0412ba7af7becb60eafef",
            "value": 20000
          }
        },
        "cc6a73cf3b204c0aa43d8b620a79dfa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e74a9cce5f4424b9ab9133edb43c241",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be39d4cedfa4464a8483865cb90cdf24",
            "value": "‚Äá20000/20000‚Äá[00:29&lt;00:00,‚Äá771.31‚Äáexamples/s]"
          }
        },
        "e225bb9f37014b968e53d90aa3c0523e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ac1b8d6eb74354a4eb481d2b832a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef199a27322647af9531d6be0a16dbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e87d04a7040e4b808bbf02914cd69838": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5052adb85ce0412ba7af7becb60eafef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e74a9cce5f4424b9ab9133edb43c241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be39d4cedfa4464a8483865cb90cdf24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79998ff89c65408f81fcc1d524991e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3b79b81060b444a887ab72c96f5e0f5",
              "IPY_MODEL_dd36b9658601403b8f91fcb2f2209da7",
              "IPY_MODEL_bccc145a51494b8aa402b4d1762b1c22"
            ],
            "layout": "IPY_MODEL_d2dee6f8a19742ee8e51e0a0e5430d11"
          }
        },
        "c3b79b81060b444a887ab72c96f5e0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_366a1084b9be425aa793cacbcd68d727",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d8409117e70e4b51baa7359c77a453fd",
            "value": "Map:‚Äá100%"
          }
        },
        "dd36b9658601403b8f91fcb2f2209da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bc0301dcfb64a859ab696f2a2650f44",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7616d6b82955427794261b488f4d3de0",
            "value": 5000
          }
        },
        "bccc145a51494b8aa402b4d1762b1c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c575db410284dc5867e4a45bde8116e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_81305b90178c475e8c544af00e794220",
            "value": "‚Äá5000/5000‚Äá[00:06&lt;00:00,‚Äá900.34‚Äáexamples/s]"
          }
        },
        "d2dee6f8a19742ee8e51e0a0e5430d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "366a1084b9be425aa793cacbcd68d727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8409117e70e4b51baa7359c77a453fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bc0301dcfb64a859ab696f2a2650f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7616d6b82955427794261b488f4d3de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c575db410284dc5867e4a45bde8116e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81305b90178c475e8c544af00e794220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d49d773b0264506a7b9de01cea41515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84d34c1b84ca401cbdecaae0e10c03e7",
              "IPY_MODEL_b0b55fb901dc417bb3d82a8f04117542",
              "IPY_MODEL_f00ebdf7935f47a5b52d822baeba457a"
            ],
            "layout": "IPY_MODEL_e967a3f3887c436a8c9e6d9c29118c55"
          }
        },
        "84d34c1b84ca401cbdecaae0e10c03e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93b184ea6e1742418eb753c36c3846c0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_34c7498f18c64daca9b737675ef18b93",
            "value": "config.json:‚Äá100%"
          }
        },
        "b0b55fb901dc417bb3d82a8f04117542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ea8d146f34405499898213d038b954",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e82a42390f8e4f04bf4b9858f182884f",
            "value": 570
          }
        },
        "f00ebdf7935f47a5b52d822baeba457a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e07049126ad44ed59bf02bc92bd8a07e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e41277945b2a40e7a474b91f23a62794",
            "value": "‚Äá570/570‚Äá[00:00&lt;00:00,‚Äá43.0kB/s]"
          }
        },
        "e967a3f3887c436a8c9e6d9c29118c55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b184ea6e1742418eb753c36c3846c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34c7498f18c64daca9b737675ef18b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33ea8d146f34405499898213d038b954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e82a42390f8e4f04bf4b9858f182884f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e07049126ad44ed59bf02bc92bd8a07e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e41277945b2a40e7a474b91f23a62794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7186a97aa114e4992b3a663261cf0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7caff86feca46b9baa7260d8bfc262e",
              "IPY_MODEL_ae79f4eebc504268bc4279feb74a3b9a",
              "IPY_MODEL_24258277f98b4bc08dfe15a2faf168b3"
            ],
            "layout": "IPY_MODEL_6811fa68fbd74e4daf5965429134a51b"
          }
        },
        "b7caff86feca46b9baa7260d8bfc262e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_646df8f2ab0843bfb8d6d240de9dc173",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3986b80c712949809829a70a20f20dc1",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "ae79f4eebc504268bc4279feb74a3b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b636f8e343b46d0a309658991cd7c36",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c264b7eed164e67bb55a6da4a59f6b5",
            "value": 440449768
          }
        },
        "24258277f98b4bc08dfe15a2faf168b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e50207c331437cbb93fd30d07d45bc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6a6229d5572e4551b87824bb7ddba3d1",
            "value": "‚Äá440M/440M‚Äá[00:05&lt;00:00,‚Äá73.6MB/s]"
          }
        },
        "6811fa68fbd74e4daf5965429134a51b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "646df8f2ab0843bfb8d6d240de9dc173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3986b80c712949809829a70a20f20dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b636f8e343b46d0a309658991cd7c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c264b7eed164e67bb55a6da4a59f6b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39e50207c331437cbb93fd30d07d45bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a6229d5572e4551b87824bb7ddba3d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}