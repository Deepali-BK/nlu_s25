{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "898baed3",
      "metadata": {
        "id": "898baed3"
      },
      "source": [
        "# HW 2: Efficient Fine-Tuning with BitFit?\n",
        "**Due: March 13, 11:30 AM**\n",
        "\n",
        "In this homework assignment, you will replicate [the BitFit experiments (Zaken et al., 2020)](https://aclanthology.org/2022.acl-short.1/). You will first use the [ðŸ¤— Transformers framework](https://huggingface.co/docs/transformers/index) to fine-tune a [BERT$_\\text{tiny}$ model](https://huggingface.co/prajjwal1/bert-tiny) ([Turc et al., 2019](https://arxiv.org/abs/1908.08962); [Bhargava et al., 2021](https://aclanthology.org/2021.insights-1.18/)) on the IMDb dataset. You will then fine-tune the same model, but with all parameters frozen other than the bias terms. You will compare the two models on the following metrics: (1) their accuracy on the IMDb test set and (2) the number of parameters trained during fine-tuning.\n",
        "\n",
        "## Important: Read Before Starting\n",
        "\n",
        "In the following exercises, you will need to implement functions defined in the `train_model.py` and `test_model.py` scripts. **Please write all your code in those files.** You should not submit this notebook with your solutions, and we will not grade it if you do. Please be aware that code written in a Jupyter notebook may run differently when copied into Python modules.\n",
        "\n",
        "The outputs shown in this notebook are the outputs that you should get **when all problems have been completed correctly**. You may obtain different results if you attempt to run the code cells before you have completed the problem set, or if you have completed one or more problems incorrectly.\n",
        "\n",
        "For part of this assignment, you will be asked to fine-tune a BERT$_\\text{tiny}$ model on the IMDb dataset with hyperparameter tuning. **This will take several hours to run on a laptop with a CPU.** You may want to instead run your code on [Google Colaboratory](https://colab.research.google.com/) using a free GPU.\n",
        "\n",
        "To begin, please run the following `import` statements."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets evaluate optuna --quiet # install datasets if it is not included in your environment"
      ],
      "metadata": {
        "id": "-lVa6aKcgLyK"
      },
      "id": "-lVa6aKcgLyK",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR27N8bsP0Rv",
        "outputId": "20c943dd-05c3-46bb-a739-3ce7186e943b"
      },
      "id": "CR27N8bsP0Rv",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOv3rEDKQH-9",
        "outputId": "2268a9f9-2aa7-435b-f8cd-4fd1605a5dae"
      },
      "id": "GOv3rEDKQH-9",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2593b3aa",
      "metadata": {
        "id": "2593b3aa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections.abc import Iterable\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Model and tokenizer from ðŸ¤— Transformers\n",
        "from transformers import AutoModelForSequenceClassification, \\\n",
        "    BertForSequenceClassification, BertTokenizerFast\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/NLU/HW2/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9hN8nSBAXPn",
        "outputId": "d66ccdae-d00d-4fe6-9f09-53530c2c37c4"
      },
      "id": "-9hN8nSBAXPn",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code you will write for this assignment\n",
        "from train_model import init_model, preprocess_dataset, init_trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "feBcOSfnTSKe",
        "outputId": "b8c33bf2-7b84-4550-b872-167c3f167e35"
      },
      "id": "feBcOSfnTSKe",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unmatched '}' (train_model.py, line 171)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-41ec15d560ae>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0;36m, in \u001b[0;35m<cell line: 0>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from train_model import init_model, preprocess_dataset, init_trainer\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/content/drive/MyDrive/NLU/HW2/train_model.py\"\u001b[0;36m, line \u001b[0;32m171\u001b[0m\n\u001b[0;31m    }\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched '}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from test_model import init_tester"
      ],
      "metadata": {
        "id": "IZgTSz0Vd3DC"
      },
      "id": "IZgTSz0Vd3DC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5110c1a8",
      "metadata": {
        "id": "5110c1a8"
      },
      "source": [
        "## Problem 1: Setup (30 Points in Total)\n",
        "\n",
        "In this assignment, you will fine-tune a pre-trained Transformer model using libraries provided by [Hugging Face](https://huggingface.co/) (whose name is usually styled using the emoji ðŸ¤—). You have already been exposed to Hugging Face in lab, where you used the [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) library to load the IMDb dataset and the [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index) library to load a pre-trained BERT$_\\text{tiny}$ model. In the following problems, additionally use the [ðŸ¤— Evaluate](https://huggingface.co/docs/evaluate/index) library, which provides evaluation metrics such as accuracy and F1.\n",
        "\n",
        "For several parts of this problem, you will need to refer to the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training) for guidance.\n",
        "\n",
        "### Problem 1a: Understand the ðŸ¤— Transformers Library (No Submission, 0 Points)\n",
        "\n",
        "ðŸ¤— Transformers is imported into Python via the name `transformers`. Please find the import statements from ðŸ¤— Transformers in the code cell above.\n",
        "\n",
        "ðŸ¤— Transformers comes with a number of different Transformer architectures, as well as [the Model Hub, a repository of pre-trained model parameters](https://huggingface.co/models). A pre-trained model is loaded by calling the model architecture's `.from_pretrained` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14fd3b9",
      "metadata": {
        "id": "e14fd3b9"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f653226",
      "metadata": {
        "id": "7f653226"
      },
      "source": [
        "The code above loads a Transformer classifier consisting of a pre-trained BERT$_\\text{base}$ encoder with case-insensitive vocabulary and a randomly initialized 2-layer MLP decoder with tanh activation. The choice of this particular set of pre-trained parameters is specified by the identifier `'bert-base-uncased'`, which is passed to the first parameter of `.from_pretrained`. Different pre-trained weights can be loaded by passing a different identifier to `.from_pretrained`. The following code loads the BERT$_\\text{tiny}$ model from [Turc et al. (2019)](https://arxiv.org/abs/1908.08962) and [Bhargava et al. (2021)](https://aclanthology.org/2021.insights-1.18/), which you will be fine-tuning in this assignment. (The `/` indicates that this is a user-submitted model, uploaded by the user [`prajjwal1`](https://huggingface.co/prajjwal1).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f491da1d",
      "metadata": {
        "id": "f491da1d"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"prajjwal1/bert-tiny\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cbed54d",
      "metadata": {
        "id": "2cbed54d"
      },
      "source": [
        "In order to load a model using the code above, you would have to know that BERT$_{\\text{tiny}}$'s architecture is implemented using the same class as BERT$_{\\text{base}}$. This is not true in general, however. For instance, if you wanted to initialize a RoBERTa classifier instead of a BERT classifier, you would need to call `RobertaForSequenceClassification.from_pretrained` instead of `BertForSequenceClassification.from_pretrained`. When you don't know which class implements the architecture of pre-trained model you want to load, you can use the `AutoModelForSequenceClassification` class ([and equivalent classes for other tasks](https://huggingface.co/docs/transformers/model_doc/auto)), which will figure out which class to instantiate based on the pre-trained weights you would like to load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cf7838",
      "metadata": {
        "id": "33cf7838"
      },
      "outputs": [],
      "source": [
        "# This code does exactly the same thing as the previous code cell\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"prajjwal1/bert-tiny\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9b9706",
      "metadata": {
        "id": "2d9b9706"
      },
      "source": [
        "In addition to models, ðŸ¤— Transformers also provides tokenizers that implement a full processing pipeline similar to what you implemented in HW 2. You can load the appropriate tokenizer for your model using a `.from_pretrained` method, just as you did with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c374e9",
      "metadata": {
        "id": "a5c374e9"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"prajjwal1/bert-tiny\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf262f25",
      "metadata": {
        "id": "cf262f25"
      },
      "source": [
        "As we saw in lab, the tokenizer object can be called as a function. Doing so will return a fully processed input, ready to be passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2388ba95",
      "metadata": {
        "id": "2388ba95"
      },
      "outputs": [],
      "source": [
        "# Because ðŸ¤— Transformers supports multiple deep learning libraries, you will\n",
        "# need to use the keyword parameter return_tensors in order to indicate that\n",
        "# you want your inputs to be returned in PyTorch format.\n",
        "inputs = tokenizer([\"Hello world!\", \"How are you?\"], padding=True,\n",
        "                   return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f381cc9",
      "metadata": {
        "id": "1f381cc9"
      },
      "source": [
        "The inputs returned by the tokenizer are passed to the model via [dictionary unpacking](https://realpython.com/python-kwargs-and-args/). The output of the model is structured, with various kinds of information provided depending on keyword arguments passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f1cba4",
      "metadata": {
        "id": "63f1cba4"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(outputs, end=\"\\n\\n\")\n",
        "\n",
        "# Use the dot operator to access parts of the output\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de67f0e9",
      "metadata": {
        "id": "de67f0e9"
      },
      "source": [
        "### Problem 1b: Understand BERT Inputs (Written, 10 Points)\n",
        "\n",
        "Look at the tokenized inputs from two code cells above. The inputs are represented as a dict with three keys: `'input_ids'`, `'token_type_ids'`, and `'attention_mask'`. What do each of those three inputs represent? Please consult the [original BERT paper (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805) for guidance.\n",
        "\n",
        "### Problem 1c: Understand BERT Hyperparameters (Written, 10 Points)\n",
        "\n",
        "For this assignment, you will perform hyperparameter tuning for the BERT$_\\text{tiny}$ model using the same procedure as in the [original paper](https://arxiv.org/abs/1908.08962). Their hyperparameter tuning procedure is documented in the [official BERT GitHub repository](https://github.com/google-research/bert) under the heading \"**\\*\\*\\*\\*\\*New March 11th, 2020: Smaller BERT Models\\*\\*\\*\\*\\***.\" Please read this documentation and describe how hyperparameter tuning was performed for the GLUE benchmark.\n",
        "\n",
        "### Problem 1d: Prepare Dataset (Code, 10 Points)\n",
        "\n",
        "As in lab, we will be using the IMDb dataset provided by ðŸ¤— Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406d2800",
      "metadata": {
        "id": "406d2800"
      },
      "outputs": [],
      "source": [
        "# Load IMDb dataset and create validation split\n",
        "imdb = load_dataset(\"imdb\")\n",
        "split = imdb[\"train\"].train_test_split(.2, seed=3463)\n",
        "imdb[\"train\"] = split[\"train\"]\n",
        "imdb[\"val\"] = split[\"test\"]\n",
        "del imdb[\"unsupervised\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a8cb0e7",
      "metadata": {
        "id": "3a8cb0e7"
      },
      "source": [
        "The ðŸ¤— Transformers fine-tuning API expects datasets to be pre-processed through the following steps.\n",
        "- All input texts should be tokenized.\n",
        "- BERT models have a maximum input length, and all inputs need to be truncated to this length.\n",
        "- Inputs shorter than the maximum input length should be padded to this length.\n",
        "- The pre-processed inputs do not need to be in the form of PyTorch tensors.\n",
        "\n",
        "These steps are performed by the `preprocess_dataset` function in `run_experiment.py`, which you will implement for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efacd457",
      "metadata": {
        "id": "efacd457"
      },
      "outputs": [],
      "source": [
        "imdb[\"train\"] = preprocess_dataset(imdb[\"train\"], tokenizer)\n",
        "imdb[\"val\"] = preprocess_dataset(imdb[\"val\"], tokenizer)\n",
        "imdb[\"test\"] = preprocess_dataset(imdb[\"test\"], tokenizer)\n",
        "\n",
        "# Visualize the preprocessed dataset\n",
        "for k, v in imdb[\"val\"][:2].items():\n",
        "    print(\"{}:\\n{}\\n{}\\n\".format(k, type(v),\n",
        "                                 [item[:20] if isinstance(item, Iterable) else\n",
        "                                 item for item in v[:5]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ac12ac",
      "metadata": {
        "id": "b8ac12ac"
      },
      "source": [
        "Please base your implementation on the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training), and please consult [Appendix A.2 of the BERT paper](https://arxiv.org/abs/1810.04805) to find out what the maximum input length should be.\n",
        "\n",
        "## Problem 2: Implement Experiment (50 Points in Total)\n",
        "### Problem 2a: Freeze Non-Bias Weights (Code, 10 Points)\n",
        "\n",
        "At the end of this assignment, you will be comparing a BERT$_{\\text{tiny}}$ model fine-tuned using BitFit to a BERT$_{\\text{tiny}}$ model fine-tuned _without_ BitFit. To run that experiment, you will need to support freezing all non-bias parameters of the model. To do this, please implement the `init_model` function, illustrated below. This function should load a pre-trained BERT classifier model from the Hugging Face Model Hub and optionally freeze non-bias parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8727100e",
      "metadata": {
        "id": "8727100e"
      },
      "outputs": [],
      "source": [
        "# The first parameter is unused; we just pass None to it\n",
        "model = init_model(None, \"prajjwal1/bert-tiny\", use_bitfit=True)\n",
        "\n",
        "# Check if weight matrix is frozen\n",
        "print(model.bert.encoder.layer[0].attention.self.query.weight.requires_grad)\n",
        "\n",
        "# Check if bias term is frozen\n",
        "print(model.bert.encoder.layer[0].attention.self.query.bias.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "173b6886",
      "metadata": {
        "id": "173b6886"
      },
      "source": [
        "**Hint:** Please consult the [documentation for the function `nn.Module.named_parameters`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters).\n",
        "\n",
        "### Problem 2b: Set Up Trainer and Tester (Code, 20 Points)\n",
        "\n",
        "ðŸ¤— Transformers provides a [`Trainer` object](https://huggingface.co/docs/transformers/main_classes/trainer) that implements training and testing a neural network. For this problem, please implement the functions `init_trainer` in `train_model.py` and `init_tester` in `test_model.py`, which will set up the `Trainer`s used to train and test your model, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06758f89",
      "metadata": {
        "id": "06758f89"
      },
      "outputs": [],
      "source": [
        "# Creates a Trainer from a Hugging Face Model Hub identifier\n",
        "trainer = init_trainer(\"prajjwal1/bert-tiny\", imdb[\"train\"], imdb[\"val\"])\n",
        "\n",
        "# Train using the trainer\n",
        "trainer.train()\n",
        "\n",
        "# Change this to whichever checkpoint you want to evalaute\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_checkpoint_directory = \"/content/checkpoints/run-1/checkpoint-5000\"\n",
        "#/content/checkpoints/run-1742097961/checkpoint-5000\n",
        "# Creates a Trainer to test a Hugging Face saved model\n",
        "tester = init_tester(eval_checkpoint_directory)"
      ],
      "metadata": {
        "id": "kHJuKtecGQAr"
      },
      "id": "kHJuKtecGQAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a67fcde8",
      "metadata": {
        "id": "a67fcde8"
      },
      "source": [
        "Your `init_trainer` function needs to support the following.\n",
        "- The training configuration (total number of epochs, early stopping criteria if any) must match your answer for Problem 1c.\n",
        "- Your `Trainer` needs to save the model obtained during each training run to a folder called `checkpoints`.\n",
        "- You should leave the `model` keyword parameter blank and instead pass an argument to the `model_init` keyword parameter.\n",
        "- It should evaluate models based on accuracy.\n",
        "\n",
        "Your `init_tester` function needs to support the following.\n",
        "- The `Trainer` should only support testing and not traiing.\n",
        "- It should evaluate models based on accuracy.\n",
        "\n",
        "\n",
        "Please use the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training) as well as [this forum post](https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378/3) for guidance. You may need to create new functions for this problem, and you may find it useful to learn about [lambda expressions](https://realpython.com/python-lambda/) if you don't know about them already.\n",
        "\n",
        "### Problem 2c: Set Up Hyperparameter Tuning (Code, 20 Points)\n",
        "\n",
        "Finally, to complete the experiment setup, you will implement hyperparameter tuning using the [Optuna](https://optuna.org/) framework. Optuna is integrated with ðŸ¤— Transformers, and it can be invoked via the `Trainer.hyperparameter_search` method. Please implement the function `hyperparameter_search_settings` in `train_model.py` by returing the correct keyword arguments for `Trainer.hyperparameter_search`. (Observe that, at the end of `train_model.py`, these keyword arguments are passed to `Trainer.hyperparameter_search` via dictionary unpacking.)  \n",
        "\n",
        "Your code should support the following requirements.\n",
        "- Your hyperparameter tuning configuration must match your answer for Problem 1c.\n",
        "- You must use Optuna for hyperparameter tuning.\n",
        "- You must indicate to Optuna that the hyperparameter search should maximize accuracy.\n",
        "\n",
        "Please use the following resources for guidance.\n",
        "- [The Hugging Face tutorial on hyperparameter tuning](https://huggingface.co/docs/transformers/hpo_train)\n",
        "- [The documentation for `Trainer.hyperparameter_search`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.Trainer.hyperparameter_search)\n",
        "- [The documentation for Optuna's `GridSampler`](https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.samplers.GridSampler.html)\n",
        "\n",
        "## Problem 3: Run Experiment (20 Points in Total)\n",
        "\n",
        "To complete the assignment, you will now run your code and report on the results. It is recommended that you run your code on [Google Colaboratory](https://colab.research.google.com/) using a free GPU.\n",
        "\n",
        "### Problem 3a: Train Models (Code and Written, 10 Points)\n",
        "\n",
        "Please now run the following experimental procedure by running `train_model.py` as a Python script:\n",
        "- first, fine-tune a BERT$_{\\text{tiny}}$ model on the IMDb dataset _with_ BitFit;\n",
        "- then, fine-tune a BERT$_{\\text{tiny}}$ model on the IMDb dataset _without_ BitFit.\n",
        "\n",
        "The `train_model.py` script should create a Pickle object containing information about the best hyperparameters found during hyperparameter tuning. Please submit this object, using the filenames `train_results_with_bitfit.p` and `train_results_without_bitfit.p` for your two training runs, respectively. Please also report the highest validation accuracy attained in each of your two training runs, as well as the hyperparameters used in those trials. Please format these results as a table such as the following.\n",
        "\n",
        "| | Validation Accuracy | Learning Rate | Batch Size |\n",
        "|---|---|---|---|\n",
        "| Without BitFit | | | |\n",
        "| With BitFit | | | |\n",
        "\n",
        "### Problem 3b: Test Models and Report Results (Code and Written, 10 Points)\n",
        "\n",
        "For each of your two training runs, please test the model that attained the best validation accuracy across all hyperparameter tuning trials. You may do so by running the `test_model.py` script. Once testing is complete, please report your results in the form of a table such as the following.\n",
        "\n",
        "| | # Trainable Parameters | Test Accuracy |\n",
        "|---|---|---|\n",
        "| Without BitFit | | |\n",
        "| With BitFit | | |\n",
        "\n",
        "The `test_model.py` script should create a Pickle object containing information about test results. Please submit this object, using the filenames `test_results_with_bitfit.p` and `test_results_without_bitfit.p` for your two tests.\n",
        "\n",
        "Finally, please comment on your results. How do they compare to the results reported by Zaken et al. (2020)? What does this say about BitFit and its applicability to other pre-trained Transformers?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKQB9N7qW0_0"
      },
      "id": "kKQB9N7qW0_0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Nak_Tp9GUOA"
      },
      "id": "3Nak_Tp9GUOA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}